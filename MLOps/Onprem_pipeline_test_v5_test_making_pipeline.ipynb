{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fc1c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T04:18:28.697510Z",
     "start_time": "2023-06-01T04:18:28.179851Z"
    }
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp import dsl\n",
    "from kfp import onprem\n",
    "from kubernetes import client as k8s_client\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from random import randrange\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "# Load environment values\n",
    "load_dotenv()\n",
    "# Create logger\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ec1ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T04:18:28.731874Z",
     "start_time": "2023-06-01T04:18:28.715838Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_persistent_volume_func(pvol_name=\"keti-shared-volume\"):\n",
    "    vop = kfp.dsl.VolumeOp(\n",
    "        name = pvol_name,\n",
    "        resource_name = pvol_name,\n",
    "        #volume_name = pvol_name,\n",
    "        size = '10Gi',\n",
    "        #modes = kfp.dsl.VOLUME_MODE_RWM,\n",
    "        #generate_unique_name = False\n",
    "    ).set_display_name(\"[0] Import Creating persistent volume\")\n",
    "    return vop\n",
    "\n",
    "def data_selection_func(pvc_args, load_data_args):#prev_cont, load_data_args):\n",
    "    data_selection_cont = dsl.ContainerOp(\n",
    "                    name=\"data_selection\",\n",
    "                    image=\"ghcr.io/keti-dp/openess-public:keti.select_dataset-0.2\", \n",
    "                    arguments=[\n",
    "                        '--selected_file_name', load_data_args[\"selected_file_name\"],\n",
    "                        '--type', load_data_args[\"ess_type\"],\n",
    "                        '--start_date',load_data_args[\"start_date\"] ,\n",
    "                        '--end_date', load_data_args[\"end_date\"] ,\n",
    "                        '--Bank', load_data_args[\"Bank\"],\n",
    "                        '--Rack', load_data_args[\"Rack\"], \n",
    "                        '--Bank_num', load_data_args[\"Bank_num\"],\n",
    "                        '--Rack_num', load_data_args[\"Rack_num\"],\n",
    "                        '--Bank_columns', load_data_args[\"Bank_columns\"], \n",
    "                        '--Rack_columns', load_data_args[\"Rack_columns\"],\n",
    "                        '--save_data_path', pvc_args.get(\"volume_mount_path\")\n",
    "                    ],\n",
    "                    command=['python', 'load_data.py'],\n",
    "                    #pvolumes={\"/data\": prev_cont.volume} # volume container\n",
    "                ).set_display_name(\"[1] Load ESS battery data\") \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return data_selection_cont\n",
    "\n",
    "def split_train_test_func(prev_cont, pvc_args, split_train_test_args, label_column):\n",
    "    split_train_test_cont = dsl.ContainerOp( # train, test 데이터 분리\n",
    "                        name=\"split train test data\",\n",
    "                        image=\"ghcr.io/keti-dp/openess-public:keti.split_train_and_test_from_dataset-0.2\",\n",
    "                        arguments=[\n",
    "                            '--load_file_name', split_train_test_args.get(\"load_file_name\"),\n",
    "                            '--load_data_path', pvc_args.get(\"volume_mount_path\"),#dsl.InputArgumentPath(data_selection_cont.outputs['data']),\n",
    "                            '--save_data_path', pvc_args.get(\"volume_mount_path\"),\n",
    "                            '--split_method', split_train_test_args,\n",
    "                            '--label_column', label_column\n",
    "                        ],\n",
    "                        command=['python', 'split_data.py'],\n",
    "                        #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                    ).set_display_name(\"[2] Split raw data to train them\").after(prev_cont) \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return split_train_test_cont\n",
    "\n",
    "class PreProcessing:\n",
    "    def anomaly_func(prev_cont, pvc_args, anomaly_detection_args):\n",
    "        anomaly_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-anomaly\",\n",
    "                                image=\"ghcr.io/keti-dp/openess-public:keti.preproc_anomaly_detection-0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--split_Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--anomaly_args', anomaly_detection_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\") #'/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_anomaly_detection.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-1] Preprocessing : Anomaly detection\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return anomaly_cont\n",
    "\n",
    "    def scaler_func(prev_cont, pvc_args, scaler_method, scaler_args):\n",
    "        scaler_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-scaler\",\n",
    "                                image=\"ghcr.io/keti-dp/openess-public:keti.preproc_scaler_models-0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--prep_method', scaler_method,\n",
    "                                    '--prep_args', scaler_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\") #'/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_scaler.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-2] Preprocessing : Scale Up & Down\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return scaler_cont\n",
    "\n",
    "class MachineLearning:\n",
    "    def regression_func(prev_cont, pvc_args, reg_method, reg_args):\n",
    "        regression_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_regresstion\",\n",
    "                                image=\"ghcr.io/keti-dp/openess-public:keti.ai_regression_models-0.2\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--regression_method', reg_method,\n",
    "                                    '--regression_args',reg_args,\n",
    "                                    '--loss_function', 'mse',\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")#+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_regresstion.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Regression method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        #.set_gpu_limit(1)\n",
    "        #ml_regression_cont.add_node_selector_constraint('nvidia')\n",
    "        return regression_cont\n",
    "\n",
    "    def classification_func(prev_cont, pvc_args, cls_method, cls_args):\n",
    "        classification_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_classification\",\n",
    "                                image=\"ghcr.io/keti-dp/openess-public:keti.ai_classification_models-0.3\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--classification_method', cls_method,\n",
    "                                    '--classification_args', cls_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")#+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_classification.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Classification method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return classification_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d478a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T04:18:28.764424Z",
     "start_time": "2023-06-01T04:18:28.755065Z"
    }
   },
   "outputs": [],
   "source": [
    "############################## SAMPLE PIPELINES ############################## \n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 1',\n",
    "    description = 'sample pipeline case 1 (Regression model)'\n",
    ")\n",
    "# Sample pipeline case #1\n",
    "def sample_pipeline_1():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    regression_cont = MachineLearning.regression_func(prev_cont=scaler_cont, \n",
    "                                                      pvc_args=pvc_args, \n",
    "                                                      reg_method=target_pl.get('reg_method'), \n",
    "                                                      reg_args=kf_params_dict.get(target_pl.get('reg_args')))\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    regression_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 2',\n",
    "    description = 'sample pipeline case 2 (Classification model)'\n",
    ")\n",
    "# Sample pipeline case #2\n",
    "def sample_pipeline_2():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    classification_cont = MachineLearning.classification_func(prev_cont=scaler_cont,\n",
    "                                                              pvc_args=pvc_args,\n",
    "                                                              cls_method=target_pl.get('cls_method'), \n",
    "                                                              cls_args=kf_params_dict.get(target_pl.get('cls_args')))\n",
    "\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    classification_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b00315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-01T04:18:29.848120Z",
     "start_time": "2023-06-01T04:18:29.154994Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KF_pipeline_log] Start time :  01/06/2023_04:18:29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/kfp/dsl/_container_op.py:1257: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://110.15.132.157:32565/pipeline/#/experiments/details/bdbc2396-d25d-41a5-a117-9685a105a8b2\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://110.15.132.157:32565/pipeline/#/runs/details/b27ebede-c666-44b8-859f-fd42dc4ed206\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load My KubeFlow Client info\n",
    "    # File path : /home/keti_iisrc/ESS_Github/ESS/.env\n",
    "    USERNAME = os.environ.get(\"USERNAME\")\n",
    "    PASSWORD = os.environ.get(\"PASSWORD\")\n",
    "    NAMESPACE = os.environ.get(\"NAMESPACE\")\n",
    "    HOST = os.environ.get(\"HOST\") # istio-ingressgateway's Node Port IP:PORT\n",
    "    \n",
    "    session = requests.Session()\n",
    "    response = session.get(HOST)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    }\n",
    "    user_data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "    session.post(response.url, headers=headers, data=user_data)\n",
    "    session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "    \n",
    "    # Connect My KubeFlow Client\n",
    "    client = kfp.Client(host=f\"{HOST}/pipeline\",\n",
    "                        namespace=f\"{NAMESPACE}\",\n",
    "                        cookies=f\"authservice_session={session_cookie}\",\n",
    "    )\n",
    "\n",
    "    # User's KF pileline file written by YAML\n",
    "    kf_params_file_dir = \"./kf_params_v2.yaml\"\n",
    "    try:\n",
    "        with open(kf_params_file_dir) as f:\n",
    "            kf_params_dict = yaml.load(f, Loader=yaml.FullLoader) #yaml.safe_load(f)\n",
    "            logger.info(\"Load KubeFlow Pipeline values from {}\", kf_params_file_dir.split(\"./\")[-1])\n",
    "            # pprint.pprint(kf_params_v1)\n",
    "    except FileNotFoundError as e:\n",
    "            logger.error(\"Failed to load KubeFlow Pipeline values from {0}\", kf_params_file_dir.split(\"./\")[-1])\n",
    "            raise e\n",
    "\n",
    "    # (example) pipeline selection\n",
    "    # load_data + split_train + anomaly_detection + reg(xgboost) + result\n",
    "    pipeline_func_name =  kf_params_dict.get('user_pipeline_func')\n",
    "    pipeline_func = eval(pipeline_func_name)\n",
    "    logger.info(\"User pipeline fuction name : \" , pipeline_func_name)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "    print(\"[KF_pipeline_log] Start time : \", dt_string)\n",
    "\n",
    "    #run_name = pipeline_func.__name__ + '-run-' + dt_string\n",
    "    #run_name = 'ML_Regression-'+pipeline_func.__name__ + '-run-' + dt_string\n",
    "    run_name = 'ML_Classification-'+pipeline_func.__name__ + '-run-' + dt_string\n",
    "    \n",
    "\n",
    "    # Compile this pipeline : pipeline_func\n",
    "    import kfp.compiler as compiler\n",
    "    compiler.Compiler().compile(pipeline_func, 'Onprem_pipeline_test_v5_cwkim_test_making_pipeline.yaml')\n",
    "    \n",
    "    # Submit pipeline directly from pipeline function\n",
    "    #arguments = {}\n",
    "    run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
    "                                                      run_name=run_name,\n",
    "                                                      namespace=NAMESPACE,\n",
    "                                                      arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022813a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-04T05:31:11.394411Z",
     "start_time": "2023-05-04T05:31:11.389524Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=7a51f592-2477-4d6e-bb2c-f3933d4a9854)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
