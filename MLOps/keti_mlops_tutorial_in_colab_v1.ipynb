{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811dca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from random import randrange\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    import kfp\n",
    "    import kfp.components as comp\n",
    "    from kfp import dsl\n",
    "    from kfp import onprem\n",
    "    from kubernetes import client as k8s_client\n",
    "except ImportError:\n",
    "    print(\"Trying to install required module : kfp \\n\")\n",
    "    os.system('python -m pip install kfp')\n",
    "    import kfp\n",
    "    import kfp.components as comp\n",
    "    from kfp import dsl\n",
    "    from kfp import onprem\n",
    "    from kubernetes import client as k8s_client\n",
    "\n",
    "#try:\n",
    "#    from dotenv import load_dotenv\n",
    "#    load_dotenv() # Load environment values\n",
    "#except ImportError:\n",
    "#    print(\"Trying to install required module : dotenv \\n\")\n",
    "#    os.system('python -m pip install python-dotenv')\n",
    "#    from dotenv import load_dotenv\n",
    "#    load_dotenv() # Load environment values\n",
    "\n",
    "# Check Python version\n",
    "!python --version\n",
    "\n",
    "# Install colab-env to use it in colab environment\n",
    "#!pip install colab-env --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To connect personal Google drive for Colab env\n",
    "# import colab_env  \n",
    "# #print(colab_env.__version__)\n",
    "# #!more /content/gdrive/MyDrive/vars.env\n",
    "# colab_env.envvar_handler.add_env(\"USERNAME\", \"\")\n",
    "# colab_env.envvar_handler.add_env(\"PASSWORD\", \"\")\n",
    "# colab_env.envvar_handler.add_env(\"NAMESPACE\", \"\")\n",
    "# colab_env.envvar_handler.add_env(\"HOST\", \"\")\n",
    "# colab_env.RELOAD()\n",
    "\n",
    "# Upload the parameters file to Colab volume\n",
    "# from google.colab import files\n",
    "# file = files.upload()\n",
    "\n",
    "# Create logger\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab946c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_persistent_volume_func(pvol_name=\"keti-shared-volume\"):\n",
    "    vop = kfp.dsl.VolumeOp(\n",
    "        name = pvol_name,\n",
    "        resource_name = pvol_name,\n",
    "        #volume_name = pvol_name,\n",
    "        size = '10Gi',\n",
    "        #modes = kfp.dsl.VOLUME_MODE_RWM,\n",
    "        #generate_unique_name = False\n",
    "    ).set_display_name(\"[0] Import Creating persistent volume\")\n",
    "    return vop\n",
    "\n",
    "def data_selection_func(pvc_args, load_data_args):#prev_cont, load_data_args):\n",
    "    data_selection_cont = dsl.ContainerOp(\n",
    "                    name=\"data_selection\",\n",
    "                    image=\"kjoohyu/data_selection:0.11\",#\"ketidp/ess:data_selection_v0.1\",\n",
    "                    arguments=[\n",
    "                        '--selected_data', load_data_args[\"selected_data\"],\n",
    "                        '--type', load_data_args[\"ess_type\"],\n",
    "                        '--start_date',load_data_args[\"start_date\"] ,\n",
    "                        '--end_date', load_data_args[\"end_date\"] ,\n",
    "                        '--Bank', load_data_args[\"Bank\"],\n",
    "                        '--Rack', load_data_args[\"Rack\"], \n",
    "                        '--Bank_num', load_data_args[\"Bank_num\"],\n",
    "                        '--Rack_num', load_data_args[\"Rack_num\"],\n",
    "                        '--Bank_columns', load_data_args[\"Bank_columns\"], \n",
    "                        '--Rack_columns', load_data_args[\"Rack_columns\"],\n",
    "                    ],\n",
    "                    command=['python', 'load_data.py'],\n",
    "                    #pvolumes={\"/data\": prev_cont.volume} # volume container\n",
    "                ).set_display_name(\"[1] Load ESS battery data\") \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return data_selection_cont\n",
    "\n",
    "def split_train_test_func(prev_cont, pvc_args, split_train_test_args, label_column):\n",
    "    split_train_test_cont = dsl.ContainerOp( # train, test 데이터 분리\n",
    "                        name=\"split train test data\",\n",
    "                        image=\"kjoohyu/split_train_test:0.12\",#\"ketidp/ess:split_train_test_v0.1\",\n",
    "                        arguments=[\n",
    "                            '--load_data_path', pvc_args.get(\"volume_mount_path\"),#dsl.InputArgumentPath(data_selection_cont.outputs['data']),\n",
    "                            '--save_data_path', pvc_args.get(\"volume_mount_path\"),\n",
    "                            '--split_method', split_train_test_args,\n",
    "                            '--label_column', label_column\n",
    "                        ],\n",
    "                        command=['python', 'split_data.py'],\n",
    "                        #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                    ).set_display_name(\"[2] Split raw data to train them\").after(prev_cont) \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return split_train_test_cont\n",
    "\n",
    "class PreProcessing:\n",
    "    def anomaly_func(prev_cont, pvc_args, anomaly_detection_args):\n",
    "        anomaly_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-anomaly\",\n",
    "                                image=\"kjoohyu/preprocessing_anomaly:0.15\",#\"ketidp/ess:preprocessing_anomaly_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--split_Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--anomaly_args', anomaly_detection_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\") #'/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_anomaly_detection.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-1] Preprocessing : Anomaly detection\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return anomaly_cont\n",
    "\n",
    "    def scaler_func(prev_cont, pvc_args, scaler_method, scaler_args):\n",
    "        scaler_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-scaler\",\n",
    "                                image=\"kjoohyu/preprocessing_scaler:0.12\",#\"ketidp/ess:preprocessing_scaler_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--prep_method', scaler_method,\n",
    "                                    '--prep_args', scaler_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\") #'/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_scaler.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-2] Preprocessing : Scale Up & Down\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return scaler_cont\n",
    "\n",
    "class MachineLearning:\n",
    "    def regression_func(prev_cont, pvc_args, reg_method, reg_args):\n",
    "        regression_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_regresstion\",\n",
    "                                image=\"kjoohyu/ml_regresstion:0.15\",#\"ketidp/ess:ml_regresstion_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--regression_method', reg_method,\n",
    "                                    '--regression_args',reg_args,\n",
    "                                    '--loss_function', 'mse',\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_regresstion.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Regression method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        #.set_gpu_limit(1)\n",
    "        #ml_regression_cont.add_node_selector_constraint('nvidia')\n",
    "        return regression_cont\n",
    "\n",
    "    def classification_func(prev_cont, pvc_args, cls_method, cls_args):\n",
    "        classification_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_classification\",\n",
    "                                image=\"kjoohyu/ml_classification:0.1\",#\"ketidp/ess:ml_classification_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--classification_method', cls_method,\n",
    "                                    '--classification_args', cls_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_classification.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Classification method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return classification_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af942009",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## SAMPLE PIPELINES ############################## \n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 1',\n",
    "    description = 'sample pipeline case 1'\n",
    ")\n",
    "# Sample pipeline case #1\n",
    "def sample_pipeline_1():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    regression_cont = MachineLearning.regression_func(prev_cont=scaler_cont, \n",
    "                                                      pvc_args=pvc_args, \n",
    "                                                      reg_method=target_pl.get('reg_method'), \n",
    "                                                      reg_args=kf_params_dict.get(target_pl.get('reg_args')))\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    regression_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 2',\n",
    "    description = 'sample pipeline case 2'\n",
    ")\n",
    "# Sample pipeline case #2\n",
    "def sample_pipeline_2():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    classification_cont = MachineLearning.classification_func(prev_cont=scaler_cont,\n",
    "                                                              pvc_args=pvc_args,\n",
    "                                                              cls_method=target_pl.get('cls_method'), \n",
    "                                                              cls_args=kf_params_dict.get(target_pl.get('cls_args')))\n",
    "\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    classification_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7132170a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load My KubeFlow Client info using colab env\n",
    "    USERNAME = \"\"\n",
    "    PASSWORD = \"\"\n",
    "    NAMESPACE = \"\"\n",
    "    HOST = \"\"\n",
    "    #USERNAME = os.getenv(\"USERNAME\")\n",
    "    #PASSWORD = os.getenv(\"PASSWORD\")\n",
    "    #NAMESPACE = os.getenv(\"NAMESPACE\")\n",
    "    #HOST = os.getenv(\"HOST\") # istio-ingressgateway's Node Port IP:PORT\n",
    "\n",
    "    session = requests.Session()\n",
    "    response = session.get(HOST)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    }\n",
    "    user_data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "    session.post(response.url, headers=headers, data=user_data)\n",
    "    session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "    \n",
    "    # Connect My KubeFlow Client\n",
    "    client = kfp.Client(host=f\"{HOST}/pipeline\",\n",
    "                        namespace=f\"{NAMESPACE}\",\n",
    "                        cookies=f\"authservice_session={session_cookie}\",\n",
    "    )\n",
    "\n",
    "    # User's KF pileline file written by YAML\n",
    "    #from google.colab import drive\n",
    "    #drive.mount('/content/gdrive')\n",
    "    #!more /content/gdrive/MyDrive/Colab_Notebooks/kf_params_v1.yaml\n",
    "    #kf_params_file_path = \"/content/gdrive/MyDrive/Colab_Notebooks/kf_params_v1.yaml\"\n",
    "    !curl -L -O https://github.com/keti-dp/OpenESS/raw/main/MLOps/kf_params_v1.yaml\n",
    "    kf_params_file_path = \"/content/kf_params_v1.yaml\" # To use the uploaded file\n",
    "    try:\n",
    "        with open(kf_params_file_path) as f:\n",
    "            kf_params_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "            logger.info(\"Load KubeFlow Pipeline values from {}\", kf_params_file_path.split(\"./\")[-1])\n",
    "            # pprint.pprint(kf_params_v1)\n",
    "    except FileNotFoundError as e:\n",
    "            logger.error(\"Failed to load KubeFlow Pipeline values from {0}\", kf_params_file_path.split(\"./\")[-1])\n",
    "            raise e\n",
    "\n",
    "    # (example) pipeline selection\n",
    "    # load_data + split_train + anomaly_detection + reg(xgboost) + result\n",
    "    pipeline_func_name =  kf_params_dict.get('user_pipeline_func')\n",
    "    pipeline_func = eval(pipeline_func_name)\n",
    "    logger.info(\"User pipeline fuction name : \" , pipeline_func_name)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "    print(\"[KF_pipeline_log] Start time : \", dt_string)\n",
    "\n",
    "    run_name = pipeline_func.__name__ + '-run-' + dt_string\n",
    "    \n",
    "\n",
    "    # Submit pipeline directly from pipeline function\n",
    "    #arguments = {}\n",
    "    run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
    "                                                      run_name=run_name,\n",
    "                                                      namespace=NAMESPACE,\n",
    "                                                      arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342d270",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_result"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
