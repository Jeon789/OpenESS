{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "74fc1c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:44.390867Z",
     "start_time": "2022-08-11T07:19:44.384544Z"
    }
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.components as comp\n",
    "from kfp import dsl\n",
    "from kfp import onprem\n",
    "from kubernetes import client as k8s_client\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from random import randrange\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import pprint\n",
    "import logging\n",
    "\n",
    "# Load environment values\n",
    "load_dotenv()\n",
    "# Create logger\n",
    "logger = logging.getLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "79ec1ca4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:44.992046Z",
     "start_time": "2022-08-11T07:19:44.974673Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_persistent_volume_func(pvol_name=\"keti-shared-volume\"):\n",
    "    vop = kfp.dsl.VolumeOp(\n",
    "        name = pvol_name,\n",
    "        resource_name = pvol_name,\n",
    "        #volume_name = pvol_name,\n",
    "        size = '10Gi',\n",
    "        #modes = kfp.dsl.VOLUME_MODE_RWM,\n",
    "        #generate_unique_name = False\n",
    "    ).set_display_name(\"[0] Import Creating persistent volume\")\n",
    "    return vop\n",
    "\n",
    "def data_selection_func(pvc_args, load_data_args):#prev_cont, load_data_args):\n",
    "    data_selection_cont = dsl.ContainerOp(\n",
    "                    name=\"data_selection\",\n",
    "                    image=\"kjoohyu/data_selection:0.11\",#\"ketidp/ess:data_selection_v0.1\",\n",
    "                    arguments=[\n",
    "                        '--selected_data', load_data_args[\"selected_data\"],\n",
    "                        '--type', load_data_args[\"ess_type\"],\n",
    "                        '--start_date',load_data_args[\"start_date\"] ,\n",
    "                        '--end_date', load_data_args[\"end_date\"] ,\n",
    "                        '--Bank', load_data_args[\"Bank\"],\n",
    "                        '--Rack', load_data_args[\"Rack\"], \n",
    "                        '--Bank_num', load_data_args[\"Bank_num\"],\n",
    "                        '--Rack_num', load_data_args[\"Rack_num\"],\n",
    "                        '--Bank_columns', load_data_args[\"Bank_columns\"], \n",
    "                        '--Rack_columns', load_data_args[\"Rack_columns\"],\n",
    "                    ],\n",
    "                    command=['python', 'load_data.py'],\n",
    "                    #pvolumes={\"/data\": prev_cont.volume} # volume container\n",
    "                ).set_display_name(\"[1] Load ESS battery data\") \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return data_selection_cont\n",
    "\n",
    "def split_train_test_func(prev_cont, pvc_args, split_train_test_args, label_column):\n",
    "    split_train_test_cont = dsl.ContainerOp( # train, test 데이터 분리\n",
    "                        name=\"split train test data\",\n",
    "                        image=\"kjoohyu/split_train_test:0.12\",#\"ketidp/ess:split_train_test_v0.1\",\n",
    "                        arguments=[\n",
    "                            '--load_data_path', pvc_args.get(\"volume_mount_path\"),#dsl.InputArgumentPath(data_selection_cont.outputs['data']),\n",
    "                            '--save_data_path', pvc_args.get(\"volume_mount_path\"),\n",
    "                            '--split_method', split_train_test_args,\n",
    "                            '--label_column', label_column\n",
    "                        ],\n",
    "                        command=['python', 'split_data.py'],\n",
    "                        #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                    ).set_display_name(\"[2] Split raw data to train them\").after(prev_cont) \\\n",
    "                .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                        volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                        volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "    return split_train_test_cont\n",
    "\n",
    "class PreProcessing:\n",
    "    def anomaly_func(prev_cont, pvc_args, anomaly_detection_args):\n",
    "        anomaly_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-anomaly\",\n",
    "                                image=\"kjoohyu/preprocessing_anomaly:0.15\",#\"ketidp/ess:preprocessing_anomaly_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--split_Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--anomaly_args', anomaly_detection_args,\n",
    "                                    '--save_data_path', '/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_anomaly_detection.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-1] Preprocessing : Anomaly detection\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return anomaly_cont\n",
    "\n",
    "    def scaler_func(prev_cont, pvc_args, scaler_method, scaler_args):\n",
    "        scaler_cont = dsl.ContainerOp(\n",
    "                                name=\"preprocessing-scaler\",\n",
    "                                image=\"kjoohyu/preprocessing_scaler:0.12\",#\"ketidp/ess:preprocessing_scaler_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--split_X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--split_X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--prep_method', scaler_method,\n",
    "                                    '--prep_args', scaler_args,\n",
    "                                    '--save_data_path', '/data'\n",
    "                                ],\n",
    "                                command=['python', 'preprocessing_scaler.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[3-2] Preprocessing : Scale Up & Down\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return scaler_cont\n",
    "\n",
    "class MachineLearning:\n",
    "    def regression_func(prev_cont, pvc_args, reg_method, reg_args):\n",
    "        regression_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_regresstion\",\n",
    "                                image=\"kjoohyu/ml_regresstion:0.15\",#\"ketidp/ess:ml_regresstion_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--regression_method', reg_method,\n",
    "                                    '--regression_args',reg_args,\n",
    "                                    '--loss_function', 'mse',\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_regresstion.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Regression method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        #.set_gpu_limit(1)\n",
    "        #ml_regression_cont.add_node_selector_constraint('nvidia')\n",
    "        return regression_cont\n",
    "\n",
    "    def classification_func(prev_cont, pvc_args, cls_method, cls_args):\n",
    "        classification_cont = dsl.ContainerOp(\n",
    "                                name=\"ml_model_classification\",\n",
    "                                image=\"kjoohyu/ml_classification:0.1\",#\"ketidp/ess:ml_classification_v0.1\",\n",
    "                                arguments=[\n",
    "                                    '--X_train', pvc_args.get(\"volume_mount_path\")+'/X_train.csv',\n",
    "                                    '--Y_train', pvc_args.get(\"volume_mount_path\")+'/Y_train.csv',\n",
    "                                    '--X_test', pvc_args.get(\"volume_mount_path\")+'/X_test.csv',\n",
    "                                    '--Y_test', pvc_args.get(\"volume_mount_path\")+'/Y_test.csv',\n",
    "                                    '--classification_method', cls_method,\n",
    "                                    '--classification_args', cls_args,\n",
    "                                    '--save_data_path', pvc_args.get(\"volume_mount_path\")+'/result.csv'\n",
    "                                ],\n",
    "                                command=['python', 'ml_classification.py'],\n",
    "                                #pvolumes={\"/data\": prev_cont.pvolume}\n",
    "                            ).set_display_name(\"[4] MachineLearning : Classification method\").after(prev_cont) \\\n",
    "                            .apply(onprem.mount_pvc(pvc_args.get(\"pvc_name\"), \n",
    "                                                    volume_name=pvc_args.get(\"volume_name\"), \n",
    "                                                    volume_mount_path=pvc_args.get(\"volume_mount_path\")))\n",
    "        return classification_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5d478a17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:45.709201Z",
     "start_time": "2022-08-11T07:19:45.698417Z"
    }
   },
   "outputs": [],
   "source": [
    "############################## SAMPLE PIPELINES ############################## \n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 1',\n",
    "    description = 'sample pipeline case 1'\n",
    ")\n",
    "# Sample pipeline case #1\n",
    "def sample_pipeline_1():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    regression_cont = MachineLearning.regression_func(prev_cont=scaler_cont, \n",
    "                                                      pvc_args=pvc_args, \n",
    "                                                      reg_method=target_pl.get('reg_method'), \n",
    "                                                      reg_args=kf_params_dict.get(target_pl.get('reg_args')))\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    regression_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(\n",
    "    name = 'Sample Pipeline 2',\n",
    "    description = 'sample pipeline case 2'\n",
    ")\n",
    "# Sample pipeline case #2\n",
    "def sample_pipeline_2():\n",
    "    target_pl = kf_params_dict.get('target_pl')\n",
    "    pvc_args = target_pl.get(\"pvc_args\")\n",
    "    #vop = create_persistent_volume_func(pvol_name=target_pl.get('persist_volume'))\n",
    "    data_selection_cont = data_selection_func(pvc_args=pvc_args,\n",
    "                                              load_data_args=target_pl.get('load_data_args'))\n",
    "    split_train_test_cont = split_train_test_func(prev_cont=data_selection_cont,\n",
    "                                                  pvc_args=pvc_args,\n",
    "                                                  split_train_test_args=target_pl.get('split_train_test_args'), \n",
    "                                                  label_column=target_pl.get('label_column'))\n",
    "    anomaly_cont = PreProcessing.anomaly_func(prev_cont=split_train_test_cont,\n",
    "                                              pvc_args=pvc_args,\n",
    "                                              anomaly_detection_args=target_pl.get('anomaly_detection_args'))\n",
    "    scaler_cont = PreProcessing.scaler_func(prev_cont=anomaly_cont,\n",
    "                                            pvc_args=pvc_args,\n",
    "                                            scaler_method=target_pl.get('scaler_method'), \n",
    "                                            scaler_args=kf_params_dict.get(target_pl.get('scaler_args')))\n",
    "    classification_cont = MachineLearning.classification_func(prev_cont=scaler_cont,\n",
    "                                                              pvc_args=pvc_args,\n",
    "                                                              cls_method=target_pl.get('cls_method'), \n",
    "                                                              cls_args=kf_params_dict.get(target_pl.get('cls_args')))\n",
    "\n",
    "    # No caching parts\n",
    "    data_selection_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    split_train_test_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    anomaly_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    scaler_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'\n",
    "    classification_cont.execution_options.caching_strategy.max_cache_staleness = 'P0D'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b00315c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:47.289023Z",
     "start_time": "2022-08-11T07:19:46.576141Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[KF_pipeline_log] Start time :  11/08/2022_07:19:47\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://1.214.41.250:32565/pipeline/#/experiments/details/bdbc2396-d25d-41a5-a117-9685a105a8b2\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://1.214.41.250:32565/pipeline/#/runs/details/a3cc783d-f2d2-414a-be9a-45c36f24f162\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load My KubeFlow Client info\n",
    "    USERNAME = os.environ.get(\"USERNAME\")\n",
    "    PASSWORD = os.environ.get(\"PASSWORD\")\n",
    "    NAMESPACE = os.environ.get(\"NAMESPACE\")\n",
    "    HOST = os.environ.get(\"HOST\") # istio-ingressgateway's Node Port IP:PORT\n",
    "    \n",
    "    session = requests.Session()\n",
    "    response = session.get(HOST)\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    }\n",
    "    user_data = {\"login\": USERNAME, \"password\": PASSWORD}\n",
    "    session.post(response.url, headers=headers, data=user_data)\n",
    "    session_cookie = session.cookies.get_dict()[\"authservice_session\"]\n",
    "    \n",
    "    # Connect My KubeFlow Client\n",
    "    client = kfp.Client(host=f\"{HOST}/pipeline\",\n",
    "                        namespace=f\"{NAMESPACE}\",\n",
    "                        cookies=f\"authservice_session={session_cookie}\",\n",
    "    )\n",
    "\n",
    "    # User's KF pileline file written by YAML\n",
    "    kf_params_file_dir = \"./kf_params_v1.yaml\"\n",
    "    try:\n",
    "        with open(kf_params_file_dir) as f:\n",
    "            kf_params_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
    "            logger.info(\"Load KubeFlow Pipeline values from {}\", kf_params_file_dir.split(\"./\")[-1])\n",
    "            # pprint.pprint(kf_params_v1)\n",
    "    except FileNotFoundError as e:\n",
    "            logger.error(\"Failed to load KubeFlow Pipeline values from {0}\", kf_params_file_dir.split(\"./\")[-1])\n",
    "            raise e\n",
    "\n",
    "    # (example) pipeline selection\n",
    "    # load_data + split_train + anomaly_detection + reg(xgboost) + result\n",
    "    pipeline_func_name =  kf_params_dict.get('user_pipeline_func')\n",
    "    pipeline_func = eval(pipeline_func_name)\n",
    "    logger.info(\"User pipeline fuction name : \" , pipeline_func_name)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%d/%m/%Y_%H:%M:%S\")\n",
    "    print(\"[KF_pipeline_log] Start time : \", dt_string)\n",
    "\n",
    "    run_name = pipeline_func.__name__ + '-run-' + dt_string\n",
    "    \n",
    "\n",
    "    # Submit pipeline directly from pipeline function\n",
    "    #arguments = {}\n",
    "    run_result = client.create_run_from_pipeline_func(pipeline_func,\n",
    "                                                      run_name=run_name,\n",
    "                                                      namespace=NAMESPACE,\n",
    "                                                      arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "022813a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:48.533474Z",
     "start_time": "2022-08-11T07:19:48.527474Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=a3cc783d-f2d2-414a-be9a-45c36f24f162)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "070bdd73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-11T07:19:51.860890Z",
     "start_time": "2022-08-11T07:19:51.854392Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#     global label_column\\n#     global load_data_args, split_train_test_args, anomaly_detection_args\\n#     global scaler_method, scaler_args\\n#     global reg_method, reg_args\\n#     global cls_method, cls_args\\n\\n    label_column = \\'RACK_SOC\\'#\\'RACK_SOC\\'\\n\\n    load_data_args = {\\n            \"start_date\": \"20211001\", \"end_date\": \"20211002\",\\n            \"ess_type\": 1, \"Bank\": False, \"Rack\": True,\\n             \"Bank_num\": 1, \"Rack_num\": 1,\\n             \"Bank_columns\": \"False\", \"Rack_columns\": \"False\"\\n        }\\n\\n    split_train_test_args = {\\n            \"size\" : 0.7, \\n            \"shuffle\": True, \\n            \"random_state\": 11\\n        }\\n\\n    anomaly_detection_args = {\\n            \"merge_test_data\" : 1, \\n            \"outlier_column\" : \"RACK_MAX_CELL_VOLTAGE\",  \\n            \"thresh_hold\" : [0.25,0.75], \\n            \"iqr_range\": 0\\n        }\\n\\n    # Scaler method\\'s args\\n    minmax_scaler_args = {\"feature_range\" : (0,1), \"copy\" : True} #, \"clip\" : False}\\n    standard_scaler_args = {\"copy\" : True, \"with_mean\" : False, \"with_std\" : True}\\n    norm_scaler_args = {\"norm\" : \"l2\", \"copy\" : True}\\n\\n    # Regression method\\'s args\\n    linear_args = { \"fit_intercept\" : True }\\n    catBoost_args = { \"iterations\" : 1000, \"learning_rate\" : 0.1, \"depth\" : 4, \"verbose\" : False }\\n    xgboost_args = {\"eta\" : 0.1, \"max_depth\" : 4} #, \"tree_method\" : \\'gpu_hist\\', \\'gpu_id\\' : 0 }\\n    lightGBM_args = {\"num_leaves\" : 15, \"learning_rate\" : 0.01 }\\n\\n    # Classification method\\'s args\\n    svm_args = { \"degree\" : 5, \"kernel\" : \\'linear\\'}\\n    randomforest_args = { \"n_estimators\" : 150}\\n\\n\\n    scaler_method = \"minmax\" # minmax, stardard, norm\\n    scaler_args = minmax_scaler_args\\n\\n    reg_method = \"xgboost\" # linear, catboost, xgboost, lightGBM\\n    reg_args = xgboost_args\\n\\n    # Not used\\n    cls_method = \"lightGBM\" # svm, randomforest, catboost, xgboost, lightGBM\\n    cls_args = lightGBM_args\\n\\n    #DATA_PATH = \\'/mnt\\'\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#     global label_column\n",
    "#     global load_data_args, split_train_test_args, anomaly_detection_args\n",
    "#     global scaler_method, scaler_args\n",
    "#     global reg_method, reg_args\n",
    "#     global cls_method, cls_args\n",
    "\n",
    "    label_column = 'RACK_SOC'#'RACK_SOC'\n",
    "\n",
    "    load_data_args = {\n",
    "            \"start_date\": \"20211001\", \"end_date\": \"20211002\",\n",
    "            \"ess_type\": 1, \"Bank\": False, \"Rack\": True,\n",
    "             \"Bank_num\": 1, \"Rack_num\": 1,\n",
    "             \"Bank_columns\": \"False\", \"Rack_columns\": \"False\"\n",
    "        }\n",
    "\n",
    "    split_train_test_args = {\n",
    "            \"size\" : 0.7, \n",
    "            \"shuffle\": True, \n",
    "            \"random_state\": 11\n",
    "        }\n",
    "\n",
    "    anomaly_detection_args = {\n",
    "            \"merge_test_data\" : 1, \n",
    "            \"outlier_column\" : \"RACK_MAX_CELL_VOLTAGE\",  \n",
    "            \"thresh_hold\" : [0.25,0.75], \n",
    "            \"iqr_range\": 0\n",
    "        }\n",
    "\n",
    "    # Scaler method's args\n",
    "    minmax_scaler_args = {\"feature_range\" : (0,1), \"copy\" : True} #, \"clip\" : False}\n",
    "    standard_scaler_args = {\"copy\" : True, \"with_mean\" : False, \"with_std\" : True}\n",
    "    norm_scaler_args = {\"norm\" : \"l2\", \"copy\" : True}\n",
    "\n",
    "    # Regression method's args\n",
    "    linear_args = { \"fit_intercept\" : True }\n",
    "    catBoost_args = { \"iterations\" : 1000, \"learning_rate\" : 0.1, \"depth\" : 4, \"verbose\" : False }\n",
    "    xgboost_args = {\"eta\" : 0.1, \"max_depth\" : 4} #, \"tree_method\" : 'gpu_hist', 'gpu_id' : 0 }\n",
    "    lightGBM_args = {\"num_leaves\" : 15, \"learning_rate\" : 0.01 }\n",
    "\n",
    "    # Classification method's args\n",
    "    svm_args = { \"degree\" : 5, \"kernel\" : 'linear'}\n",
    "    randomforest_args = { \"n_estimators\" : 150}\n",
    "\n",
    "\n",
    "    scaler_method = \"minmax\" # minmax, stardard, norm\n",
    "    scaler_args = minmax_scaler_args\n",
    "\n",
    "    reg_method = \"xgboost\" # linear, catboost, xgboost, lightGBM\n",
    "    reg_args = xgboost_args\n",
    "\n",
    "    # Not used\n",
    "    cls_method = \"lightGBM\" # svm, randomforest, catboost, xgboost, lightGBM\n",
    "    cls_args = lightGBM_args\n",
    "\n",
    "    #DATA_PATH = '/mnt'\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fea0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
